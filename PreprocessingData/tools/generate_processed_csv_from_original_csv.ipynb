{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# This script is for generating training dataset from \n",
    "# original .csv file by standardizing product_type and \n",
    "# labelling the first class label by algorithms.\n",
    "#\n",
    "# Please make sure the input file path and type is correct\n",
    "# Also, make sure product_to_all.json and main_to_num.json\n",
    "# in the same directory(or same path.) Since these two files\n",
    "# are the standardize information we use and the product label \n",
    "# map to number we predefined.\n",
    "# \n",
    "# Author: Luis Lin\n",
    "# Date: June 27, 2022\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy \n",
    "from difflib import SequenceMatcher\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from string_grouper import match_strings, match_most_similar\n",
    "import numpy as np\n",
    "import text_cleaner as tc\n",
    "import re\n",
    "#the main_categories with all sub_categories dict\n",
    "products_to_all = {}\n",
    "#the main_categories relate to numbers dict\n",
    "main_categories_map_to_num = {}\n",
    "#all categories of product including the name of main_cat\n",
    "all_cat = set()\n",
    "#each specific item maps to the main_categories_number\n",
    "specific_products_map_to_num = {}\n",
    "\n",
    "'''\n",
    "Input: two separate strings\n",
    "Function:\n",
    "    We provide another statistic model (ML) from spacy.similarity to compare two strings similarity firstly.\n",
    "    if spacy similarities doesn't not exit like 0.0, then we compare two strings similarity by \"gestalt pattern matching\" \n",
    "    not_Statistic model(ML)  It is a character-based matcher. \n",
    "Output: Similarity [0, 1] between those two strings\n",
    "'''\n",
    "def similar(a: str, b: str) -> float:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc1 = nlp(a)\n",
    "    doc2 = nlp(b)\n",
    "    statistical_method_score = doc1.similarity(doc2)\n",
    "    non_statistical_method_score = SequenceMatcher(None, a, b).ratio()\n",
    "    if statistical_method_score < 0.1:\n",
    "        return non_statistical_method_score\n",
    "    return statistical_method_score\n",
    "\n",
    "'''\n",
    "Input: string\n",
    "Function: remove any plural format and wired suffix, for example, dresses -> ['dress']\n",
    "Output: list \n",
    "'''\n",
    "def lemma_string(original_string:str) -> List:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(original_string.lower())\n",
    "    temp = []\n",
    "    for token in doc:\n",
    "        if token.text == '&' or token.text == 'and' or token.text == '/' or token.text == ',':\n",
    "            continue\n",
    "        if token.text == \"glasses\" or token.text == 'booty' or token.text == \"sunglasses\" or token.text == \"earrings\": #some of them should be represented in plural format\n",
    "            temp.append(token.text)\n",
    "        else:\n",
    "            temp.append(token.lemma_)\n",
    "    return temp\n",
    "'''\n",
    "input: original string\n",
    "function: remove pre_suffix of the original 'product_type' to match more accurate items\n",
    "output: modified string\n",
    "'''\n",
    "def modify_product_type(original:str):\n",
    "    if len(original) <= 1:\n",
    "        return original\n",
    "    removable_words = [\"clothing\", \"sale\", \"man\", \"men\",\"mens\", \"men's\", \"woman\", \"women\", \"womens\", \"women's\",  \"unisex\",\"girl\", \"girls\", \"girl's\", \"lady\", \"ladies\", \"snow\",\"ladies'\", \"active\",\"boy\", \"boys\", \"boy's\", \"graphic\", \"premium\", \"cozy\", \"designer\",\"comfort\", \"athletic\",\"casual\", 'youth', 'adult']\n",
    "    original = original.replace(\"+\", \" \").replace(\"-\", \" \").replace(\"&\", \" \").replace(\"/\", \" \").replace(\",\", \" \")\n",
    "    original = re.compile(r\"\\s+\").sub(\" \", original).strip()\n",
    "    ori_list = original.split(\" \")\n",
    "    \n",
    "    acc = ['acc', 'accessory', 'acessories','accesssories','accessories', \"jewelry\",\"polarized\", \"non polar\"]\n",
    "    shoes = ['footwear']\n",
    "    homeware = [\"home\", \"homeware\"]\n",
    "    tops = [\"top\", \"tops\"]\n",
    "    main_cat = ''\n",
    "    sub_cat = []\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for e in ori_list:\n",
    "        e = e.strip().lower()\n",
    "        if e in removable_words:\n",
    "            continue\n",
    "        elif not res.__contains__(e):\n",
    "            res.append(e)\n",
    "        if e in acc:\n",
    "            main_cat = \"accessories\"\n",
    "        elif e in shoes:\n",
    "            main_cat = \"shoes\"\n",
    "        elif e in tops:\n",
    "            main_cat = \"tops\"\n",
    "        elif e == \"bottoms\":\n",
    "            main_cat = \"bottoms\"\n",
    "        elif e in homeware:\n",
    "            main_cat = \"homeware\"\n",
    "        elif e == \"beauty\":\n",
    "            main_cat = \"beauty\"\n",
    "        else:\n",
    "            sub_cat.append(e)\n",
    "    #if main_cat doesn't exist but sub_cat exists, we can try to map directly by specific_product_map\n",
    "    sub_cat = \" \".join(sub_cat)\n",
    "    \n",
    "    return \" \".join(res), main_cat, sub_cat\n",
    "\n",
    "\n",
    "'''\n",
    "Input: None\n",
    "Function: Read the .json file to initilize the dictionaries and sets\n",
    "Output: None\n",
    "'''\n",
    "def initiailize_containers() -> None:\n",
    "    with open(\"json_files/product_to_all.json\") as f1:\n",
    "        global products_to_all \n",
    "        products_to_all = json.load(f1)\n",
    "    with open(\"json_files/main_categories_to_num.json\") as f2: \n",
    "        global main_categories_map_to_num\n",
    "        main_categories_map_to_num = json.load(f2)\n",
    "    with open(\"json_files/specific_product_map_to_num.json\") as f3:\n",
    "        global specific_products_map_to_num\n",
    "        specific_products_map_to_num = json.load(f3)\n",
    "    for key in specific_products_map_to_num.keys():\n",
    "        all_cat.add(key)\n",
    "'''\n",
    "Input: dataframe\n",
    "Function: to calculate some statistics data after pre-processing.\n",
    "'''\n",
    "def summary_of_the_new_df(df:pd.DataFrame)->None:\n",
    "    unknown = 0\n",
    "    totalnum = df.shape[0]\n",
    "    shoes = 0\n",
    "    other_clothing = 0\n",
    "    tops = 0\n",
    "    bottoms = 0\n",
    "    beauty = 0\n",
    "    home = 0\n",
    "    acc = 0\n",
    "    other = 0\n",
    "    for row in range(df.shape[0]):\n",
    "        n = df.loc[row, \"label_1st\"]\n",
    "        if n == 0:\n",
    "            unknown += 1\n",
    "        elif n == 1:\n",
    "            shoes += 1\n",
    "        elif n == 2:\n",
    "            tops += 1\n",
    "        elif n == 3:\n",
    "            bottoms += 1\n",
    "        elif n == 4:\n",
    "            other_clothing += 1\n",
    "        elif n == 5:\n",
    "            beauty += 1\n",
    "        elif n == 6:\n",
    "            acc += 1\n",
    "        elif n == 7:\n",
    "            home += 1\n",
    "        elif n == 8:\n",
    "            other += 1\n",
    "        \n",
    "    print(\"unknown\", unknown, \"\\t\\tratio of all\", unknown/totalnum)\n",
    "    total = totalnum - unknown\n",
    "    print(\"\\nshoes\", shoes,\"\\t\\tratio\", \"{:10.2f}\".format(shoes/total))\n",
    "    print(\"tops\", tops, \"\\t\\tratio\", \"{:10.2f}\".format(tops/total))\n",
    "    print(\"bottoms\", bottoms, \"\\t\\tratio\", \"{:10.2f}\".format(bottoms/total))\n",
    "    print(\"other_clothing\", other_clothing,\"\\tratio\", \"{:10.2f}\".format(other_clothing/total))\n",
    "    print(\"beauty\", beauty, \"\\t\\tratio\", \"{:10.2f}\".format(beauty/total))\n",
    "    print(\"accessories\", acc, \"\\tratio\", \"{:10.2f}\".format(acc/total))\n",
    "    print(\"homeware\", home, \"\\t\\tratio\", \"{:10.2f}\".format(home/total))\n",
    "    print(\"other\", other, \"\\t\\tratio\", \"{:10.2f}\".format(other/total))\n",
    "#iniyilize all maps and sets.\n",
    "initiailize_containers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_path = \"/Users/luis/Downloads/products-June-28th.csv\"\n",
    "assert original_file_path != None\n",
    "columns = [\"id\", \"title\", \"tags\", \"images\", \"gender\",\"product_type\",  \"colors\", \"buckets\", \"url\", \"body_html\"]\n",
    "df = pd.DataFrame()\n",
    "try:\n",
    "    df = pd.read_csv(original_file_path,  usecols=columns).reset_index()\n",
    "except:\n",
    "    print(\"The path seems incorrect\")\n",
    "#insert new columns to this df which are useful and processed information \n",
    "\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"product_type(modified)\", \"\", allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"main_category\", \"\", allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"sub_category\", \"\", allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"match_most_similar_>80%_string\", \"\", allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"match_most_similar_>60%_string\", \"\", allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"label_1st\", 0, allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"label_2nd\", 0, allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"gender\"), \"label_3rd\", 0, allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"buckets\"), \"buckets_num\", 0, allow_duplicates=True)\n",
    "df.insert(df.columns.get_loc(\"url\"), \"color_num\", 0, allow_duplicates=True)\n",
    "df = df[[\"index\",\"id\", \"title\", \"tags\", \"images\", \"gender\",\"product_type\", \"product_type(modified)\", \"main_category\", \"sub_category\", \"match_most_similar_>80%_string\", \"match_most_similar_>60%_string\", \"label_1st\", \"label_2nd\",\"label_3rd\", \"buckets_num\", \"buckets\", \"color_num\", \"colors\", \"url\", \"body_html\"]]\n",
    "df.insert(df.columns.get_loc(\"body_html\"), \"raw_text\", \"\", allow_duplicates=True)\n",
    "for i in range(df.shape[0]):\n",
    "    \n",
    "    ori_word = df.loc[i, 'product_type']\n",
    "    title = df.loc[i, 'title']\n",
    "    tags = df.loc[i, 'tags']\n",
    "    buckets = df.loc[i, 'buckets']\n",
    "    body_html = df.loc[i, 'body_html']\n",
    "    \n",
    "    if not isinstance(ori_word, str):\n",
    "        df.loc[i,['product_type']] = \"unknown\"\n",
    "        df.loc[i, ['product_type(modified)']] = \"unknown\"\n",
    "        continue\n",
    "    #get the modify_product_type (remove)\n",
    "    try:\n",
    "        product_type_new, main_cat, sub_cat= modify_product_type(ori_word)\n",
    "        df.loc[i, ['product_type(modified)']] = product_type_new\n",
    "        df.loc[i, [\"main_category\"]] = main_cat\n",
    "        df.loc[i, [\"sub_category\"]] = sub_cat\n",
    "        \n",
    "        if (main_cat != \"\"):\n",
    "            #label the 1st, label if we already know exact main_catgories. \n",
    "            df.loc[i,[\"label_1st\"]] = main_categories_map_to_num.get(main_cat)\n",
    "    except:\n",
    "        print(i, ori_word)\n",
    "    try:\n",
    "        #clean body_html as well. \n",
    "        df.loc[i, 'raw_text'] = tc.cleanHtml(body_html)\n",
    "    except:\n",
    "        print(\"Something wrong with clean html: \", i)\n",
    "del(df[\"body_html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#build data series from all categories.\n",
    "pre_defiened_labels = pd.Series(list(all_cat), name=\"pre_defined_label\")\n",
    "#get 80% most_similar_mathes(dataframe) by using package string-matcher\n",
    "most_similar_matches = match_most_similar( pre_defiened_labels, df[\"sub_category\"],\\\n",
    "    min_similarity = 0.80, ignore_index=False, replace_na=False)\n",
    "most_similar_matches = pd.concat([df['index'],df[\"sub_category\"], most_similar_matches], axis=1)\n",
    "#get 60% most_similar_matches(dataframe) \n",
    "less_similar_matches = match_strings(pre_defiened_labels, df[\"sub_category\"],\\\n",
    "    min_similarity = 0.65, ignore_index = False, replace_na = False)\n",
    "#fill up 80% most_similar_80% column\n",
    "empty_cells = 0\n",
    "for row in range(most_similar_matches.shape[0]):\n",
    "    index = most_similar_matches.loc[row, \"index\"]\n",
    "    most_similar_pre_defined_label = most_similar_matches.loc[row, 'most_similar_pre_defined_label']\n",
    "    most_similar_index = most_similar_matches.loc[row, ['most_similar_index']].item()\n",
    "    \n",
    "    if not np.isnan(most_similar_index):\n",
    "        df.loc[df['index'] == index, ['match_most_similar_>80%_string']] = most_similar_pre_defined_label\n",
    "    else:\n",
    "        empty_cells += 1\n",
    "#fill up 60% less_similar_60% column\n",
    "index_similarity_map = {}\n",
    "index_to_pre_map = {}\n",
    "for row in range(less_similar_matches.shape[0]):\n",
    "    current_index = less_similar_matches.loc[row, 'right_index']\n",
    "    label = less_similar_matches.loc[row, 'left_pre_defined_label']\n",
    "    similarity = less_similar_matches.loc[row, 'similarity']\n",
    "    \n",
    "    if index_similarity_map.get(current_index) == None:\n",
    "        index_similarity_map.update({current_index:similarity})\n",
    "        index_to_pre_map.update({current_index:label})\n",
    "    elif index_similarity_map.get(current_index) >= similarity:\n",
    "        index_similarity_map.update({current_index:similarity})\n",
    "        index_to_pre_map.update({current_index:label})\n",
    "    else: # similarity < then current, ignore\n",
    "        continue\n",
    "for key in index_to_pre_map.keys():\n",
    "    df.loc[df['index'] == key, ['match_most_similar_>60%_string']] = index_to_pre_map.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r2/xt2ymfnd7nz2khk687zqlrpr0000gn/T/ipykernel_27443/2676033653.py:31: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  statistical_method_score = doc1.similarity(doc2)\n"
     ]
    }
   ],
   "source": [
    "#Algorithm 2: if both 60% and 80% don't exist, just ignore, \n",
    "#             if the 80% string doesn't exist, compare the 60% string with 80% string similarity by diff() and spacy(), \n",
    "#             let spacy.similarity to decide whether above 60% to fill the 80%. if spacy similarity does not exist, fill \n",
    "#             fill up 80% by similar() algorithm. \n",
    "#     speed : 500 data around 1 mins based on M1 pro.\n",
    "double_check_pairs_pt_word60_dic = {}\n",
    "#see the accurancy \n",
    "\n",
    "def is_nan_string(string):\n",
    "    return len(string) == 0\n",
    "#w = df.loc[66, [\"match_most_similar_>80%_string\"]].item()\n",
    "#print(w)\n",
    "#is_nan_string(w)\n",
    "for row in range(df.shape[0]):\n",
    "    word_80 = df.loc[row, \"match_most_similar_>80%_string\"]\n",
    "    word_60 = df.loc[row, \"match_most_similar_>60%_string\"]\n",
    "    #print(is_nan_string(word_80), \"   \",word_60)\n",
    "    if is_nan_string(word_80) and is_nan_string(word_60):\n",
    "        #found nothing\n",
    "        continue    \n",
    "    elif is_nan_string(word_80) and not is_nan_string(word_60):\n",
    "        pt_modified = df.loc[row, \"sub_category\"]\n",
    "        pair = (pt_modified, word_60)\n",
    "        #print(pair)\n",
    "        if pair not in double_check_pairs_pt_word60_dic.keys():\n",
    "            \n",
    "            double_check_pairs_pt_word60_dic.update({pair: similar(word_60, pt_modified)})\n",
    "        #print(\"product_type(modified) \", pt_modified, \" words(60%): \", word_60, \" Similarity: \", similar(word_60, pt_modified))\n",
    "        if similar(word_60, pt_modified) > 0.0:\n",
    "            df.loc[row, \"match_most_similar_>80%_string\"] = word_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('blanket towel', 'blanket'): 0.7409514024113583,\n",
       " ('dresses sun dresses', 'dress'): 0.8874078855687398,\n",
       " ('dresses', 'dress'): 1.0000000571365557,\n",
       " ('denim', 'denim short'): 0.8061123004322047,\n",
       " ('swim wear one piece', 'one piece'): 0.7537377263726195,\n",
       " ('hats', 'hat'): 1.0000000929979673,\n",
       " ('sunglasses sunglasses acetateframe non polar',\n",
       "  'sunglasses'): 0.8361596445974069,\n",
       " ('masks', 'mask'): 0.9999999509638685,\n",
       " ('clogs', 'clog'): 0.8888888888888888,\n",
       " ('sunglasses sunglasses metal frame', 'sunglasses'): 0.8023430109343745,\n",
       " ('sunglasses sunglasses acetate frame', 'sunglasses'): 0.7872220237976908,\n",
       " ('leather bags', 'leather'): 0.7887546956095955,\n",
       " ('bags', 'bag'): 0.9999999574708032,\n",
       " ('bottom', 'bikini bottom'): 0.7010592044148047,\n",
       " ('jeans', 'jean'): 0.18565857977597314,\n",
       " ('shoe', 'shoes'): 1.0000000267699771,\n",
       " ('pullover', 'pullover hoodie'): 0.8388999417205346,\n",
       " ('sunglasses sunglasses acetate frame non polar',\n",
       "  'sunglasses'): 0.6967328885980686,\n",
       " ('jumpsuits rompers', 'jumpsuit romper'): 0.2896266698301745,\n",
       " ('short sleeve shirt', 'short sleeve'): 0.8570800770463408,\n",
       " ('parka jacket', 'parka'): 0.7876842982630964,\n",
       " ('denim blazer', 'blazer'): 0.9999999697786466,\n",
       " ('short sleeve shirts', 'short sleeve'): 0.8652936777884702,\n",
       " ('caps', 'cap'): 0.2507941536654137,\n",
       " ('bomber', 'bomber jacket'): 0.7937984294683181,\n",
       " (\"kimono's\", 'kimono'): 0.7932996564000868,\n",
       " ('straw', 'straw hat'): 1.0000000929979673,\n",
       " ('bralette bikini', 'bralette'): 0.696836563546304,\n",
       " ('fashion one pieces', 'one piece'): 0.6448954726402903,\n",
       " ('tankini bikini', 'tankini'): 0.7983146926295613,\n",
       " ('tees', 'tee'): 0.8571428571428571,\n",
       " ('body lotions', 'hand body lotion'): 0.8874188166477579,\n",
       " ('tanks', 'tank'): 0.15613156630452035,\n",
       " ('headwear headwear hats snap back', 'headwear'): 0.7813339700369761,\n",
       " ('cocktails', 'cocktail dress'): 0.4654833117992505,\n",
       " ('headwear headwear hats mesh back', 'headwear'): 0.8067632654210828,\n",
       " ('the hipster', 'hipst'): 0.625,\n",
       " ('bags leather', 'leather'): 0.7887546956095955,\n",
       " ('cover scarf', 'scarf'): 0.7767272329046524,\n",
       " ('cover shawl', 'shawl'): 0.7767272329046524,\n",
       " ('bags canvas', 'canvas'): 0.7652609727802703,\n",
       " ('headwear headwear hats beach', 'headwear'): 0.8477440724462342,\n",
       " ('sunglasses sunglasses acetateframe prizm',\n",
       "  'sunglasses'): 0.8346845916241379,\n",
       " ('pants leggings', 'legging'): 0.197313228778295,\n",
       " ('bags wallets', 'wallet'): 0.9999999574708032,\n",
       " ('shoes flat shoes', 'flat shoe'): 0.9594859669380574,\n",
       " ('short sleeve t shirt', 'short sleeve'): 0.7539494655932761,\n",
       " ('ties pocket squares', 'tie pocket square'): 0.7098704397530178,\n",
       " ('sunglasses sunglasses metal frame non polar',\n",
       "  'sunglasses'): 0.7047237246009268,\n",
       " ('bags bags shoulder bags', 'shoulder bag'): 0.9333555426793004,\n",
       " ('sunglasses sunglasses wire frame', 'sunglasses'): 0.7949472013104716,\n",
       " ('wallet wallets card cases', 'wallet'): 0.8597850724364053,\n",
       " ('underwear underwear boxer briefs tech', 'underwear'): 0.9329074472387073,\n",
       " ('headwear headwear hats five panel', 'headwear'): 0.7679853910792587,\n",
       " ('socks socks high', 'sock'): 0.9386939809689092,\n",
       " ('socks socks', 'sock'): 1.0000000004203768,\n",
       " ('watches watches', 'watch'): 0.21185678398868243,\n",
       " ('leather band', 'leather band watch'): 0.8578620574170774,\n",
       " ('underwear underwear boxer briefs cotton', 'underwear'): 0.9335806048770826,\n",
       " ('sunglasses sunglasses acetateframe', 'sunglasses'): 0.9999999200140428,\n",
       " ('tote', 'tote bag'): 0.9999999574708032,\n",
       " ('underwear underwear boxer briefs', 'underwear'): 0.9619393705659145,\n",
       " ('sunglasses sunglasses metal frame prizm', 'sunglasses'): 0.7361382345774297,\n",
       " ('sunglasses sunglasses wire frame non polar',\n",
       "  'sunglasses'): 0.7125774892879837,\n",
       " ('short sleeve t shirts', 'short sleeve'): 0.7809055400146295,\n",
       " ('rompers jumpsuits', 'jumpsuit romper'): 0.2896266698301745,\n",
       " ('short sleeve button up shirts', 'button-up shirt'): 0.6128854137966951,\n",
       " ('long sleeve t shirts', 'long sleeve'): 0.7809055400146295,\n",
       " ('longboard wheels', 'longboard'): 0.659500883529472,\n",
       " ('polos', 'polo'): 1.0000000089549863,\n",
       " ('vests', 'vest'): 0.18939607925766822,\n",
       " ('snowboard tuning', 'snowboard'): 0.6723163947213142,\n",
       " ('longboard trucks', 'longboard'): 0.6470973535269563,\n",
       " ('winter face masks', 'face mask'): 0.9274835384619358,\n",
       " ('long sleeve button up shirts', 'button-up shirt'): 0.6128854137966951,\n",
       " ('wakesurf bags', 'wakesurfs'): 0.8181818181818182,\n",
       " ('snowboard boots', 'snowboard'): 0.6802402007241638,\n",
       " ('snowboard bags', 'snowboard'): 0.7194465166030924,\n",
       " ('toddler base layers', 'base layer'): 0.8258885307153246,\n",
       " ('snowboard socks', 'snowboard'): 0.703462449310874,\n",
       " ('watches', 'watch'): 0.21185678398868243,\n",
       " ('low wedge', 'wedge'): 0.7790288057287426,\n",
       " ('low platform', 'platform'): 0.8307027933192994,\n",
       " ('pack', 'backpack'): 0.6666666666666666,\n",
       " ('flat', 'flat shoe'): 0.7092593486047062,\n",
       " ('high wedge', 'wedge'): 0.7790288057287426,\n",
       " ('sandle', 'candle'): 0.10362143556812971,\n",
       " ('kids sleepwear', 'sleepwear'): 0.8573243766261134,\n",
       " ('kids swimwear', 'swimwear'): 0.8573243766261134,\n",
       " ('sweetlegs plus', 'sweetleg'): 0.7272727272727273,\n",
       " ('sweetlegs kids', 'sweetleg'): 0.7272727272727273,\n",
       " ('button up short sleeve', 'button-up shirt'): 0.6857997349783528,\n",
       " ('tee short sleeve', 'short sleeve'): 0.8570800767069842,\n",
       " ('mugs drinkware', 'drinkware'): 0.8435008383000124,\n",
       " ('accessories: cologne', 'accessories'): 0.6322073646834055,\n",
       " ('skateboards', 'skateboard deck'): 0.1400997204021604,\n",
       " ('boots', 'boot'): 0.15054492969995684,\n",
       " ('tee long sleeve', 'long sleeve'): 0.8570800767069842}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_check_pairs_pt_word60_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From previous we finished match similar products. We start to label the \"label_1st\" column\n",
    "for row in range(df.shape[0]):\n",
    "    key = df.loc[row, 'match_most_similar_>80%_string']\n",
    "    if key != None and specific_products_map_to_num.get(key) != None:\n",
    "        df.loc[row, ['label_1st']] = specific_products_map_to_num.get(key)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"look.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown 7945 \t\tratio of all 0.2794976430028847\n",
      "\n",
      "shoes 1303 \t\tratio       0.06\n",
      "tops 6033 \t\tratio       0.29\n",
      "bottoms 3764 \t\tratio       0.18\n",
      "other_clothing 1251 \tratio       0.06\n",
      "beauty 403 \t\tratio       0.02\n",
      "accessories 5985 \tratio       0.29\n",
      "homeware 691 \t\tratio       0.03\n",
      "other 1051 \t\tratio       0.05\n"
     ]
    }
   ],
   "source": [
    "summary_of_the_new_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processed_products_from_June28.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f33c96bfe48976e0772c3a4097c2fff4477ea59ec4556e2e8fa1251315c612"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
