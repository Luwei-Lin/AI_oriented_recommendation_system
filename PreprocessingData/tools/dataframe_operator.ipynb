{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Panda to concat, delete, insert, loc the data and I/O .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. from tops_total.csv to operate some modifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tops_total_modified.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/luis/Documents/GitHub/2022Summer/AI-Oriented-Recommendation-System/PreprocessingData/tools/dataframe_operator.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/2022Summer/AI-Oriented-Recommendation-System/PreprocessingData/tools/dataframe_operator.ipynb#ch0000003?line=0'>1</a>\u001b[0m tops_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mtops_total_modified.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=571'>572</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=573'>574</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=574'>575</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=576'>577</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=929'>930</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=931'>932</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=932'>933</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1212'>1213</a>\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1213'>1214</a>\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1214'>1215</a>\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1215'>1216</a>\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1216'>1217</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1217'>1218</a>\u001b[0m     f,\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1218'>1219</a>\u001b[0m     mode,\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1219'>1220</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1220'>1221</a>\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1221'>1222</a>\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1222'>1223</a>\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1223'>1224</a>\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1224'>1225</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1225'>1226</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1226'>1227</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1227'>1228</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=783'>784</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=784'>785</a>\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=785'>786</a>\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=786'>787</a>\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=787'>788</a>\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=788'>789</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=789'>790</a>\u001b[0m             handle,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=790'>791</a>\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=791'>792</a>\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=792'>793</a>\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=793'>794</a>\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=794'>795</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=795'>796</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=796'>797</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/luis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py?line=797'>798</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tops_total_modified.csv'"
     ]
    }
   ],
   "source": [
    "tops_df = pd.read_csv('tops_total_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "#if we want to modify the row contain some errors with different names\n",
    "i = tops_df[(tops_df.product_type == \"cloak\")].index\n",
    "tops_df = tops_df.drop(i)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "j = tops_df[((tops_df.product_type == \"dress\") | (tops_df.product_type == \"dresses\") | (tops_df.product_type == \"dresses - formal\"))].index\n",
    "print(j)\n",
    "tops_df = tops_df.drop(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOther operation for dataframes. \\nsince I put some files to different location, so the codes below just used once for creating tops_total.csv\\n#from all tops data we have to build the total dataset\\ntops_df_1 = pd.read_csv(\\'train_data/tops.csv\\', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])  \\ntops_df_2 = pd.read_csv(\\'sample.csv\\', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])\\ntops_df = pd.concat([tops_df_1, tops_df_2], ignore_index=True)\\ntops_df.to_csv(\\'train_data/tops_total.csv\\')\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Other operation for dataframes. \n",
    "since I put some files to different location, so the codes below just used once for creating tops_total.csv\n",
    "#from all tops data we have to build the total dataset\n",
    "tops_df_1 = pd.read_csv('train_data/tops.csv', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])  \n",
    "tops_df_2 = pd.read_csv('sample.csv', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])\n",
    "tops_df = pd.concat([tops_df_1, tops_df_2], ignore_index=True)\n",
    "tops_df.to_csv('train_data/tops_total.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "k = tops_df[((tops_df.product_type == \"short\") | (tops_df.product_type == \"shorts\"))].index\n",
    "print(k)\n",
    "tops_df  = tops_df.drop(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([213], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "l = tops_df[(tops_df.product_type == \"sweetlegs maternity\")].index\n",
    "print(l)\n",
    "tops_df = tops_df.drop(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean all unnessary rows and columns, we output the modified tops.csv\n",
    "tops_df = tops_df.loc[:, [\"title\",  \"product_type\", \"tags\", \"body_html\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_df.to_csv('tops.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create raw Train_data (.csv) and Test_data (.csv) from our total dataset (e.g 223 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build 180 train_data and 44 test_data 80% train 20% test for tops 223 samples\n",
    "tops_df_train = tops_df.loc[0:180, [\"title\",  \"product_type\", \"tags\", \"body_html\"]].copy()\n",
    "tops_df_test = tops_df.loc[180:,  [\"title\",  \"product_type\", \"tags\", \"body_html\"]].copy()\n",
    "tops_df_train.to_csv('train_data/tops_train.csv', index= False)\n",
    "tops_df_test.to_csv('test_data/tops_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Customized DataSet From product.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aria High Waist; d cup+ meta size chart artesands size guide moderate coverage one piece plus size swim wear. The Hues Underwire One Piece has a beautifully sculpted feminine wrap-around style. The Hue has a hidden shelf bra with internal support underwire to fit a D cup to DD cup. This swimsuit style allows all of the fit and support required for body sculpting and figure forming confidence for the curvy body. Model is wearing an Australian size 18+ FEATURES:Supportive Underwire  Removable and Adjustable Straps  Front Ruching D/DD Cup Underwire Support  Nylon / Elastane Blend. Artesands Fits Your Curves. Designed in Australia. Care NotesWe recommend hand washing in cold water. Dry flat in a shady spot out of direct sunlight. Do not wring out or hang dry. Chlorinated water is not great and can cause colours to fade. Please make sure you always wash your swimsuit well after swimming in chlorinated water. We recommend swimsuit cleaner to keep your swimsuit looking amazing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>body_html</th>\n",
       "      <th>product_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7609644417242</td>\n",
       "      <td>Aria High Waist</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6750552359068</td>\n",
       "      <td>Hues Underwire One Piece</td>\n",
       "      <td>{d-cup+,meta-size-chart-artesands-size-guide,m...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;&lt;meta charset=\"utf-...</td>\n",
       "      <td>one piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7552737738970</td>\n",
       "      <td>Marble Maya One Piece</td>\n",
       "      <td>{meta-size-chart-bound-size-guide,moderate-cov...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p data-mce-fragment=\"...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7552752550106</td>\n",
       "      <td>Marble Malibu Set</td>\n",
       "      <td>{crop,meta-size-chart-bound-size-guide,moderat...</td>\n",
       "      <td>&lt;p data-mce-fragment=\"1\"&gt;&lt;span&gt;A Bond-Eye orig...</td>\n",
       "      <td>bikini top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5918371152028</td>\n",
       "      <td>Sophia Bottom</td>\n",
       "      <td>{color-patterned,high-waist,meta-size-chart-po...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;Meet your newest gi...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>612668932154</td>\n",
       "      <td>Lion 22\" Bronze</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5918535942300</td>\n",
       "      <td>Sonja Top</td>\n",
       "      <td>{color-stripes,crop,meta-size-chart-power-2-th...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;This retro revival ...</td>\n",
       "      <td>bikini top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6613956362396</td>\n",
       "      <td>Shirley Tie-Side</td>\n",
       "      <td>{high-waist,meta-size-chart-power-2-the-flower...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;&lt;meta charset=\"utf-...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5918251450524</td>\n",
       "      <td>Gwen Ruffle Bottom</td>\n",
       "      <td>{color-patterned,high-leg,meta-size-chart-powe...</td>\n",
       "      <td>&lt;p class=\"p1\" data-mce-fragment=\"1\"&gt;Gwen Ruffl...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5815675879580</td>\n",
       "      <td>Billie Bottom</td>\n",
       "      <td>{color-patterned,meta-size-chart-power-2-the-f...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p data-mce-fragment=\"...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                     title  \\\n",
       "0  7609644417242           Aria High Waist   \n",
       "1  6750552359068  Hues Underwire One Piece   \n",
       "2  7552737738970     Marble Maya One Piece   \n",
       "3  7552752550106         Marble Malibu Set   \n",
       "4  5918371152028             Sophia Bottom   \n",
       "5   612668932154           Lion 22\" Bronze   \n",
       "6  5918535942300                 Sonja Top   \n",
       "7  6613956362396          Shirley Tie-Side   \n",
       "8  5918251450524        Gwen Ruffle Bottom   \n",
       "9  5815675879580             Billie Bottom   \n",
       "\n",
       "                                                tags  \\\n",
       "0                                                 {}   \n",
       "1  {d-cup+,meta-size-chart-artesands-size-guide,m...   \n",
       "2  {meta-size-chart-bound-size-guide,moderate-cov...   \n",
       "3  {crop,meta-size-chart-bound-size-guide,moderat...   \n",
       "4  {color-patterned,high-waist,meta-size-chart-po...   \n",
       "5                                                 {}   \n",
       "6  {color-stripes,crop,meta-size-chart-power-2-th...   \n",
       "7  {high-waist,meta-size-chart-power-2-the-flower...   \n",
       "8  {color-patterned,high-leg,meta-size-chart-powe...   \n",
       "9  {color-patterned,meta-size-chart-power-2-the-f...   \n",
       "\n",
       "                                           body_html   product_type  \n",
       "0                                                NaN            NaN  \n",
       "1  <meta charset=\"utf-8\">\\n<p><meta charset=\"utf-...      one piece  \n",
       "2  <meta charset=\"utf-8\">\\n<p data-mce-fragment=\"...            NaN  \n",
       "3  <p data-mce-fragment=\"1\"><span>A Bond-Eye orig...     bikini top  \n",
       "4  <meta charset=\"utf-8\">\\n<p>Meet your newest gi...  bikini bottom  \n",
       "5                                                NaN        default  \n",
       "6  <meta charset=\"utf-8\">\\n<p>This retro revival ...     bikini top  \n",
       "7  <meta charset=\"utf-8\">\\n<p><meta charset=\"utf-...  bikini bottom  \n",
       "8  <p class=\"p1\" data-mce-fragment=\"1\">Gwen Ruffl...  bikini bottom  \n",
       "9  <meta charset=\"utf-8\">\\n<p data-mce-fragment=\"...  bikini bottom  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from text_cleaner import cleanHtml\n",
    "from text_cleaner import clean_product_description\n",
    "\n",
    "columnChosen = ['id', 'title', 'product_type','tags', 'body_html']\n",
    "products = pd.read_csv(\"/Users/luis/Downloads/products.csv\", usecols=columnChosen)\n",
    "products.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = products[((products.product_type.isnull()) | (products.product_type == 'default') \n",
    "            | (products.body_html.isnull()) | (products.tags == '{}') \n",
    "            | (products.product_type == 'none'))].index\n",
    "products = products.drop(k).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop any null and default rows in product_type and drop empty 'tags' rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17973 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#pre-clean products cloumns and build our own data set columns and input-data format for NLP.\n",
    "len(products.index)\n",
    "r, c = products.shape[0], products.shape[1]\n",
    "print(r, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.insert(len(products.columns), '1st_class_label', 0, allow_duplicates=True)\n",
    "products.insert(len(products.columns), '2nd_class_label', 0, allow_duplicates=True)\n",
    "products.insert(len(products.columns), '3rd_class_label', 0, allow_duplicates=True )\n",
    "products.insert(len(products.columns), 'raw_text', \"\", allow_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_cleaner import raw_content\n",
    "for i in range(products.shape[0]):\n",
    "    \n",
    "    products.loc[i, 'raw_text'] = raw_content(products.loc[i, 'title'], products.loc[i, 'tags'], products.loc[i, 'body_html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('temp_not_finished.csv')\n",
    "k = products[((products.product_type.isnull()) | (products.product_type == 'default') \n",
    "            | (products.body_html.isnull()) | (products.tags == '{}') \n",
    "            | (products.product_type == 'none'))].index\n",
    "products = products.drop(k).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17973, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.to_csv('temp_not_finished.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### next step: 1_st_class_label, semi_automated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([], dtype='int64')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "products = pd.read_csv(\"temp_not_finished.csv\")\n",
    "k = products[((products.product_type.isnull()) | (products.product_type == 'default') \n",
    "            | (products.body_html.isnull()) | (products.tags == '{}') \n",
    "            | (products.product_type == 'none'))].index\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "set = {\"bikini top\"}\n",
    "for i in range(products.shape[0]):\n",
    "    set.add(products.loc[i, 'product_type'])\n",
    "len(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import wordnet31 as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Path_similarity Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1. By default, there is now a fake root node added to verbs so for cases where previously a path could not be found—and None was returned—it should return a value. The old behavior can be achieved by setting simulate_root to be False. A score of 1 represents identity i.e. comparing a sense with itself will return 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jean = wn.synset('jeans.n.01')\n",
    "pants = wn.synset('pants.n.01')\n",
    "jean.path_similarity(pants)\n",
    "wn.path_similarity(jean, pants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Leacock-Chodorow Similarity: Return a score denoting how similar two word senses are, based on the shortest path that connects the senses (as above) and the maximum depth of the taxonomy in which the senses occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.845826690498331"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Leacock-Chodorow Similarity, two word senses are\n",
    "wn.lch_similarity(jean, pants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "WordNetError",
     "evalue": "no lemma 'hoodie' with part of speech 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1439\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1439\u001b[0m     offset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lemma_pos_offset_map[lemma][pos][synset_index]\n\u001b[1;32m   1440\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'n'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/luis/Documents/GitHub/AI-Oriented-Recommendation-System/PreprocessingData/tools/dataframe_operator.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/AI-Oriented-Recommendation-System/PreprocessingData/tools/dataframe_operator.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hoodie \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49msynset(\u001b[39m'\u001b[39;49m\u001b[39mhoodie.n.01\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/AI-Oriented-Recommendation-System/PreprocessingData/tools/dataframe_operator.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m fleece \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39msynset(\u001b[39m'\u001b[39m\u001b[39mfleece.n.01\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/AI-Oriented-Recommendation-System/PreprocessingData/tools/dataframe_operator.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m wn\u001b[39m.\u001b[39mlch_similarity(hoodie, fleece)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1442\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1441\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mno lemma \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with part of speech \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1442\u001b[0m     \u001b[39mraise\u001b[39;00m WordNetError(message \u001b[39m%\u001b[39m (lemma, pos)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1444\u001b[0m     n_senses \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lemma_pos_offset_map[lemma][pos])\n",
      "\u001b[0;31mWordNetError\u001b[0m: no lemma 'hoodie' with part of speech 'n'"
     ]
    }
   ],
   "source": [
    "hoodie = wn.synset('hoodie.n.01')\n",
    "fleece = wn.synset('fleece.n.01')\n",
    "wn.lch_similarity(hoodie, fleece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Wu-Palmer Similarity: Return a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782608695652174"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wup_similarity(pants, jean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wup_similarity(dog, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Resnik Similarity: Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node). Note that for any similarity measure that uses information content, the result is dependent on the corpus used to generate the information content and the specifics of how the information content was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat') \n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "from nltk.corpus import genesis\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e+300"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.res_similarity(dog, cat, semcor_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.197538297317934"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dog.res_similarity(cat, genesis_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.675174315069589"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jean.res_similarity(pants, genesis_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.675174315069589"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.res_similarity(jean,pants, genesis_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Jiang-Conrath Similarity Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-300"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pants.jcn_similarity(jean, genesis_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Lin Similarity: Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lin_similarity(jean, pants, semcor_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.lin_similarity(dog, semcor_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#', 'product_type', 'Spacy_similarity', 'nltk_similarity',\n",
       "       'product_type(modified)', 'main_category', 'sub_category', 'match_80',\n",
       "       'match_60', 'label_1st', 'label_2nd', 'label_3rd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47316317002575875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36363636363636365"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib # not good at semantic similarity\n",
    "sm = difflib.SequenceMatcher(None)\n",
    "sm.set_seq1(\"hat\")\n",
    "sm.set_seq2(\"headwear\")\n",
    "sm.ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('./json_files/label3_to_label2.json')\n",
    "l3_l2 = json.load(f)\n",
    "our_compared_list_l2 = set()\n",
    "our_compared_list_l3 = set()\n",
    "for k, v in l3_l2.items():\n",
    "    our_compared_list_l2.add(v)\n",
    "    our_compared_list_l3.add(k)\n",
    "our_compared_list_l2 = list(our_compared_list_l2)\n",
    "our_compared_list_l3 = list(our_compared_list_l3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumpsuit\n",
      "bodycon dress\n",
      "handheld bag\n",
      "hair primer\n",
      "camis\n",
      "rectangular glasses\n",
      "short dress\n",
      "skateboard helmet\n",
      "nutrition\n",
      "training\n",
      "sweatshirt\n",
      "coats jackets\n",
      "office shirt\n",
      "book\n",
      "eyebrow\n",
      "hat\n",
      "lip balm\n",
      "lip balm treatment\n",
      "tote bag\n",
      "denim jackets\n",
      "technical sport jackets\n",
      "dry shampoo\n",
      "phone case\n",
      "poncho\n",
      "ring\n",
      "eyeliner\n",
      "golf\n",
      "drop tee\n",
      "button-up shirt\n",
      "short sleeve\n",
      "leather jackets\n",
      "set spray\n",
      "knitwear\n",
      "jelly\n",
      "stilettos\n",
      "dad jean\n",
      "artwork\n",
      "beauty tool\n",
      "scarf\n",
      "slipper\n",
      "cargos pant\n",
      "glove\n",
      "heel boot\n",
      "lip\n",
      "pajama set\n",
      "eye cream\n",
      "moisturizer\n",
      "drinkware\n",
      "denim skirt\n",
      "swim top\n",
      "bra\n",
      "dress\n",
      "v neck\n",
      "sweetleg\n",
      "parkas\n",
      "body wash\n",
      "blue light block glasses\n",
      "air freshener\n",
      "flat iron\n",
      "platform heel\n",
      "suit\n",
      "lingerie\n",
      "the hipster\n",
      "hair mask\n",
      "culotte jean\n",
      "graphic tee\n",
      "flip flop\n",
      "tee\n",
      "conditioner\n",
      "nail\n",
      "espadrille\n",
      "cardigan\n",
      "leave-in conditioner\n",
      "bralette\n",
      "clog\n",
      "area rug\n",
      "skateboard deck\n",
      "shower curtain\n",
      "cocktail dress\n",
      "highlighter\n",
      "blanket\n",
      "mock neck\n",
      "board short\n",
      "run\n",
      "nightgown\n",
      "t-shirt\n",
      "turtle neck\n",
      "earrings\n",
      "long sleeve\n",
      "platform boot\n",
      "rain boot\n",
      "card\n",
      "chelsea boot\n",
      "straightener\n",
      "perfume\n",
      "bath bomb\n",
      "pyrrha\n",
      "flat shoe\n",
      "madra short\n",
      "face serum\n",
      "wedge heel\n",
      "concealer\n",
      "crop pant\n",
      "toque\n",
      "bath robe\n",
      "necklace\n",
      "clip\n",
      "skateboard wheel\n",
      "square glasses\n",
      "dress shirt\n",
      "wallet\n",
      "croptop\n",
      "puzzle\n",
      "croptee\n",
      "wide leg pant\n",
      "set powder\n",
      "hair spray\n",
      "slide\n",
      "snowboard\n",
      "cycling short\n",
      "other\n",
      "blouse\n",
      "duvet cover\n",
      "flare pant\n",
      "hair serum\n",
      "leather band watch\n",
      "jacket\n",
      "home\n",
      "pantie\n",
      "cologne\n",
      "sweatpant\n",
      "swimsuit\n",
      "crossbody bag\n",
      "baby\n",
      "leather suede\n",
      "backpack\n",
      "chinos pant\n",
      "underwear\n",
      "skin care\n",
      "sweet top\n",
      "jackets\n",
      "skirt\n",
      "crop hoodie\n",
      "denim short\n",
      "headphone\n",
      "cargo short\n",
      "fleece jackets\n",
      "coat\n",
      "raincoat\n",
      "tie\n",
      "flare jean\n",
      "hair comb\n",
      "cleanser\n",
      "bikini bottom\n",
      "skate shoe\n",
      "pump\n",
      "kitten heel\n",
      "lip gloss\n",
      "tankini\n",
      "hair oil\n",
      "skort skirt\n",
      "sandal\n",
      "slim pant\n",
      "high top sneaker\n",
      "crop jean\n",
      "chunky sandal\n",
      "foundation\n",
      "snowshoe\n",
      "lace-up boot\n",
      "sock\n",
      "zip up hoodie\n",
      "ankle strap heel\n",
      "swim short\n",
      "sleeveless\n",
      "pant\n",
      "hair brush\n",
      "jersey\n",
      "bracelet\n",
      "exfoliator\n",
      "bottle\n",
      "bikini top\n",
      "waterproof boot\n",
      "sneaker\n",
      "face brush\n",
      "watch\n",
      "bronzer\n",
      "activewear\n",
      "ice gripper boot\n",
      "curl iron\n",
      "mom jean\n",
      "nail care\n",
      "camisole\n",
      "button-down shirt\n",
      "gear\n",
      "lip brush\n",
      "cover up\n",
      "home decoration\n",
      "tunic\n",
      "legging\n",
      "pant trouser\n",
      "one piece\n",
      "skateboard\n",
      "eyeshadow\n",
      "skateboard wax\n",
      "cloak\n",
      "shirt\n",
      "wakesurf\n",
      "bikini\n",
      "skinny jean\n",
      "mask\n",
      "sleeveless hooodie\n",
      "hoodie\n",
      "sweater vest\n",
      "sunscreen\n",
      "tank top\n",
      "booty short\n",
      "bag\n",
      "ceramic\n",
      "cosmetic\n",
      "soap\n",
      "heel wedge\n",
      "anklet\n",
      "read glasses\n",
      "long dress\n",
      "umbrella\n",
      "swimwear\n",
      "baggy jean\n",
      "brush cleaner\n",
      "sponge\n",
      "skateboard truck\n",
      "hand lotion\n",
      "denim jogger\n",
      "quilt jackets\n",
      "suede\n",
      "t\n",
      "headband\n",
      "swim\n",
      "cap\n",
      "pullover hoodie\n",
      "lounge\n",
      "intimate\n",
      "pocket square\n",
      "shawl\n",
      "pillow\n",
      "hair care\n",
      "straight pant\n",
      "body lotion\n",
      "jogger\n",
      "snow helmelt\n",
      "aviator glasses\n",
      "jewelry\n",
      "clock\n",
      "cat eye glasses\n",
      "oxford loafer\n",
      "swim bottom\n",
      "woodcarve\n",
      "face mask\n",
      "slip-on sneaker\n",
      "mitt\n",
      "heel\n",
      "panty\n",
      "canvas sneaker\n",
      "deodorant\n",
      "down jackets\n",
      "crop shirt\n",
      "styling cream\n",
      "shoulder bag\n",
      "flatcap\n",
      "food\n",
      "sport bra\n",
      "nail polish\n",
      "base layer\n",
      "heat protectant\n",
      "capri\n",
      "swimwear cleaner\n",
      "collar\n",
      "jean\n",
      "towel\n",
      "toner\n",
      "midi skirt\n",
      "belt\n",
      "robe\n",
      "metal band watch\n",
      "cami top\n",
      "crop top\n",
      "midi dress\n",
      "headwear\n",
      "shampoo\n",
      "ankle boot\n",
      "textile\n",
      "sport set\n",
      "scalp scrub\n",
      "sleepwear\n",
      "tall boot\n",
      "raverback tank\n",
      "mule\n",
      "false eyelash\n",
      "applicator\n",
      "snap back\n",
      "trucker hat\n",
      "culotte\n",
      "bomber jackets\n",
      "stationery\n",
      "heel cover\n",
      "curvy jean\n",
      "straw hat\n",
      "hair\n",
      "skort short\n",
      "polo\n",
      "snapback hat\n",
      "hair dryer\n",
      "evening bag\n",
      "candle\n",
      "rip jean\n",
      "trouser\n",
      "sleepshirt\n",
      "makeup remover\n",
      "sunglasses\n",
      "eye primer\n",
      "shower gel\n",
      "canvas\n",
      "wedge\n",
      "glasses\n",
      "kimono\n",
      "mascara\n",
      "winter boot\n",
      "vest\n",
      "classic sneaker\n",
      "long skirt\n",
      "eye brush\n",
      "romper\n",
      "longboard\n",
      "blazer suit\n",
      "blazer\n",
      "bandana\n",
      "semi-slim pant\n",
      "brooch\n",
      "waistcoat\n",
      "run short\n",
      "face primer\n",
      "tank\n",
      "face wipe\n",
      "sticker\n",
      "clutch minaudiere\n",
      "sun hat\n",
      "sleeve\n",
      "straight jean\n",
      "tight\n",
      "round glasses\n",
      "lip treatment\n",
      "swim trunk\n",
      "yoga\n",
      "athletic boxer\n",
      "skateboard tool\n",
      "baseball hat\n",
      "scrunchie\n",
      "canteen\n",
      "jean legge\n",
      "shoe lace\n",
      "brush set\n",
      "boot\n",
      "tee shirt\n",
      "falt sandal\n",
      "platform\n",
      "denim\n",
      "contour\n",
      "crop sweater\n",
      "bootcut jean\n",
      "short\n",
      "blush\n",
      "wool jackets\n",
      "sweater\n",
      "lipstick\n",
      "bodysuit\n",
      "short skirt\n",
      "high-rise jean\n",
      "fedora\n",
      "bath mat\n"
     ]
    }
   ],
   "source": [
    "for item in our_compared_list_l3:\n",
    "    print(nlp(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r2/xt2ymfnd7nz2khk687zqlrpr0000gn/T/ipykernel_69756/2180732399.py:18: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  sim = doc1.similarity(doc2)\n",
      "/var/folders/r2/xt2ymfnd7nz2khk687zqlrpr0000gn/T/ipykernel_69756/2180732399.py:27: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  sim = doc1.similarity(doc2)\n"
     ]
    }
   ],
   "source": [
    "#Using spacy to find cloest similarity categories(Brute Force)\n",
    "import spacy\n",
    "import pandas as pd\n",
    "products = pd.read_csv(\"../data/product_types.csv\")\n",
    "products.insert(2, 'Spacy_similarity_layer2', '', allow_duplicates=True)\n",
    "products.insert(3, 'Spacy_similarity_layer3', '', allow_duplicates=True)\n",
    "products.insert(4, 'nltk_tfidf_cos_similarity_layer2', '', allow_duplicates=True)\n",
    "products.insert(4, 'nltk_tfidf_cos_similarity_layer3', '', allow_duplicates=True)\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "for i in range(0,products.shape[0]):\n",
    "    pt = products.loc[i]['product_type']\n",
    "    #calulate pt with all product similarity\n",
    "    distance = {}#our_label: distance_with_pt\n",
    "    for l3 in our_compared_list_l3:\n",
    "        doc1 = nlp(pt)\n",
    "        doc2 = nlp(l3)\n",
    "        sim = doc1.similarity(doc2)\n",
    "        distance[l3] = sim\n",
    "    sorted_distance = list(dict(sorted(distance.items(), key=lambda item: item[1], reverse=True)))\n",
    "    first = sorted_distance[0]\n",
    "    products.loc[i, 'Spacy_similarity_layer3'] = first\n",
    "    distance_2 = {}#our_label: distance_with_pt\n",
    "    for l2 in our_compared_list_l2:\n",
    "        doc1 = nlp(pt)\n",
    "        doc2 = nlp(l2)\n",
    "        sim = doc1.similarity(doc2)\n",
    "        distance_2[l2] = sim\n",
    "    sorted_distance_2 = list(dict(sorted(distance_2.items(), key=lambda item: item[1], reverse=True)))\n",
    "    first = sorted_distance_2[0]\n",
    "    products.loc[i, 'Spacy_similarity_layer2'] = first\n",
    "\n",
    "\n",
    "\n",
    "    #sort by value of dictionary put the first one to the spacy_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.to_csv(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosmetics': 0, 'scarf': 1, 'blazer/suit': 2, 'ties': 3, 'boot': 4, 'clip': 5, 'sweaters': 6, 'watch': 7, 'underwear': 8, 'sneakers': 9, 'glove': 10, 'homeware': 11, 'headwear': 12, 'care': 13, 'nail': 14, 'sleeve': 15, 'lace': 16, 'shoe': 17, 'bag': 18, 'scrunchie': 19, 'tank': 20, 'tops': 21, 'belt': 22, 'perfume': 23, 'skateboard': 24, 'leggings': 25, 'shirts': 26, 'tight': 27, 'wallet': 28, 'glasses': 29, 'heel/wedge': 30, 'pocket': 31, 'square': 32, 'short': 33, 'dresses': 34, 'skirts': 35, 'jumpsuits': 36, 'jewelry': 37, 'activewear': 38, 'pants/trousers': 39, 'coats/jackets': 40, 'bandana': 41, 'bath': 42, 'robe': 43, 'sock': 44, 'others': 45, 'sandal': 46, 'lingerie': 47, 'leather': 48, 'suede': 49, 'hair': 50, 'sleepwear': 51, 'beauty': 52, 'tools': 53, 'skin': 54, 'sticker': 55, 'slipper': 56, 'cloak': 57, 'swimwear': 58}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "#Tokenize words and create dictionary\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in our_compared_list_l2]\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "\n",
    "#print(dictionary.token2id)\n",
    "\n",
    "#create a bad of words (Corpus)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08f33c96bfe48976e0772c3a4097c2fff4477ea59ec4556e2e8fa1251315c612"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
