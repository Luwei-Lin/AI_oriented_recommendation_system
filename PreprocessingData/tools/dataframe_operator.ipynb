{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Panda to concat, delete, insert, loc the data and I/O .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. from tops_total.csv to operate some modifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_df = pd.read_csv('tops_total_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "#if we want to modify the row contain some errors with different names\n",
    "i = tops_df[(tops_df.product_type == \"cloak\")].index\n",
    "tops_df = tops_df.drop(i)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "j = tops_df[((tops_df.product_type == \"dress\") | (tops_df.product_type == \"dresses\") | (tops_df.product_type == \"dresses - formal\"))].index\n",
    "print(j)\n",
    "tops_df = tops_df.drop(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOther operation for dataframes. \\nsince I put some files to different location, so the codes below just used once for creating tops_total.csv\\n#from all tops data we have to build the total dataset\\ntops_df_1 = pd.read_csv(\\'train_data/tops.csv\\', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])  \\ntops_df_2 = pd.read_csv(\\'sample.csv\\', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])\\ntops_df = pd.concat([tops_df_1, tops_df_2], ignore_index=True)\\ntops_df.to_csv(\\'train_data/tops_total.csv\\')\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Other operation for dataframes. \n",
    "since I put some files to different location, so the codes below just used once for creating tops_total.csv\n",
    "#from all tops data we have to build the total dataset\n",
    "tops_df_1 = pd.read_csv('train_data/tops.csv', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])  \n",
    "tops_df_2 = pd.read_csv('sample.csv', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\"])\n",
    "tops_df = pd.concat([tops_df_1, tops_df_2], ignore_index=True)\n",
    "tops_df.to_csv('train_data/tops_total.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "k = tops_df[((tops_df.product_type == \"short\") | (tops_df.product_type == \"shorts\"))].index\n",
    "print(k)\n",
    "tops_df  = tops_df.drop(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([213], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "l = tops_df[(tops_df.product_type == \"sweetlegs maternity\")].index\n",
    "print(l)\n",
    "tops_df = tops_df.drop(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean all unnessary rows and columns, we output the modified tops.csv\n",
    "tops_df = tops_df.loc[:, [\"title\",  \"product_type\", \"tags\", \"body_html\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_df.to_csv('tops.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create raw Train_data (.csv) and Test_data (.csv) from our total dataset (e.g 223 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build 180 train_data and 44 test_data 80% train 20% test for tops 223 samples\n",
    "tops_df_train = tops_df.loc[0:180, [\"title\",  \"product_type\", \"tags\", \"body_html\"]].copy()\n",
    "tops_df_test = tops_df.loc[180:,  [\"title\",  \"product_type\", \"tags\", \"body_html\"]].copy()\n",
    "tops_df_train.to_csv('train_data/tops_train.csv', index= False)\n",
    "tops_df_test.to_csv('test_data/tops_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Customized DataSet From product.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>body_html</th>\n",
       "      <th>product_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aria High Waist</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hues Underwire One Piece</td>\n",
       "      <td>{d-cup+,meta-size-chart-artesands-size-guide,m...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;&lt;meta charset=\"utf-...</td>\n",
       "      <td>one piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marble Maya One Piece</td>\n",
       "      <td>{meta-size-chart-bound-size-guide,moderate-cov...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p data-mce-fragment=\"...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marble Malibu Set</td>\n",
       "      <td>{crop,meta-size-chart-bound-size-guide,moderat...</td>\n",
       "      <td>&lt;p data-mce-fragment=\"1\"&gt;&lt;span&gt;A Bond-Eye orig...</td>\n",
       "      <td>bikini top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sophia Bottom</td>\n",
       "      <td>{color-patterned,high-waist,meta-size-chart-po...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;Meet your newest gi...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lion 22\" Bronze</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sonja Top</td>\n",
       "      <td>{color-stripes,crop,meta-size-chart-power-2-th...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;This retro revival ...</td>\n",
       "      <td>bikini top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shirley Tie-Side</td>\n",
       "      <td>{high-waist,meta-size-chart-power-2-the-flower...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p&gt;&lt;meta charset=\"utf-...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gwen Ruffle Bottom</td>\n",
       "      <td>{color-patterned,high-leg,meta-size-chart-powe...</td>\n",
       "      <td>&lt;p class=\"p1\" data-mce-fragment=\"1\"&gt;Gwen Ruffl...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Billie Bottom</td>\n",
       "      <td>{color-patterned,meta-size-chart-power-2-the-f...</td>\n",
       "      <td>&lt;meta charset=\"utf-8\"&gt;\\n&lt;p data-mce-fragment=\"...</td>\n",
       "      <td>bikini bottom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0           Aria High Waist   \n",
       "1  Hues Underwire One Piece   \n",
       "2     Marble Maya One Piece   \n",
       "3         Marble Malibu Set   \n",
       "4             Sophia Bottom   \n",
       "5           Lion 22\" Bronze   \n",
       "6                 Sonja Top   \n",
       "7          Shirley Tie-Side   \n",
       "8        Gwen Ruffle Bottom   \n",
       "9             Billie Bottom   \n",
       "\n",
       "                                                tags  \\\n",
       "0                                                 {}   \n",
       "1  {d-cup+,meta-size-chart-artesands-size-guide,m...   \n",
       "2  {meta-size-chart-bound-size-guide,moderate-cov...   \n",
       "3  {crop,meta-size-chart-bound-size-guide,moderat...   \n",
       "4  {color-patterned,high-waist,meta-size-chart-po...   \n",
       "5                                                 {}   \n",
       "6  {color-stripes,crop,meta-size-chart-power-2-th...   \n",
       "7  {high-waist,meta-size-chart-power-2-the-flower...   \n",
       "8  {color-patterned,high-leg,meta-size-chart-powe...   \n",
       "9  {color-patterned,meta-size-chart-power-2-the-f...   \n",
       "\n",
       "                                           body_html   product_type  \n",
       "0                                                NaN            NaN  \n",
       "1  <meta charset=\"utf-8\">\\n<p><meta charset=\"utf-...      one piece  \n",
       "2  <meta charset=\"utf-8\">\\n<p data-mce-fragment=\"...            NaN  \n",
       "3  <p data-mce-fragment=\"1\"><span>A Bond-Eye orig...     bikini top  \n",
       "4  <meta charset=\"utf-8\">\\n<p>Meet your newest gi...  bikini bottom  \n",
       "5                                                NaN        default  \n",
       "6  <meta charset=\"utf-8\">\\n<p>This retro revival ...     bikini top  \n",
       "7  <meta charset=\"utf-8\">\\n<p><meta charset=\"utf-...  bikini bottom  \n",
       "8  <p class=\"p1\" data-mce-fragment=\"1\">Gwen Ruffl...  bikini bottom  \n",
       "9  <meta charset=\"utf-8\">\\n<p data-mce-fragment=\"...  bikini bottom  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from text_cleaner import cleanHtml\n",
    "from text_cleaner import clean_product_description\n",
    "\n",
    "columnChosen = ['title', 'product_type','tags', 'body_html']\n",
    "products = pd.read_csv(\"/Users/luis/Downloads/products.csv\", usecols=columnChosen)\n",
    "products.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = products[((products.product_type.isnull()) | (products.product_type == 'default') \n",
    "            | (products.body_html.isnull()) | (products.tags == '{}'))].index\n",
    "products = products.drop(k).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18053 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#pre-clean products cloumns and build our own data set columns and input-data format for NLP.\n",
    "len(products.index)\n",
    "r, c = products.shape[0], products.shape[1]\n",
    "print(r, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.insert(len(products.columns), '1_st_class_label', 0, allow_duplicates=True)\n",
    "products.insert(len(products.columns), 'raw_text', \"\", allow_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_cleaner import raw_content\n",
    "for i in range(products.shape[0]):\n",
    "    \n",
    "    products.loc[i, 'raw_text'] = raw_content(products.loc[i, 'title'], products.loc[i, 'tags'], products.loc[i, 'body_html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.to_csv('temp_not_finished.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08f33c96bfe48976e0772c3a4097c2fff4477ea59ec4556e2e8fa1251315c612"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
