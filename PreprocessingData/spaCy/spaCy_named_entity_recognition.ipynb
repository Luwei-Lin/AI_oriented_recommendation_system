{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pastel fundamental t; nu psychedelic\n",
      "(t,)\n",
      "(t,)\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f1449c1054364307a8e0524e6e5c9774-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">pastel</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">fundamental</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">t;</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">nu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">psychedelic</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f1449c1054364307a8e0524e6e5c9774-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f1449c1054364307a8e0524e6e5c9774-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f1449c1054364307a8e0524e6e5c9774-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f1449c1054364307a8e0524e6e5c9774-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f1449c1054364307a8e0524e6e5c9774-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f1449c1054364307a8e0524e6e5c9774-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f1449c1054364307a8e0524e6e5c9774-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f1449c1054364307a8e0524e6e5c9774-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import common as c\n",
    "\n",
    "content_test = c.clean_tags_text(\"PASTEL FUNDAMENTAL T\", None, '{Nu,Psychedelic}')\n",
    "print(content_test)\n",
    "nlp_stat = spacy.load(\"ML_based_model\")\n",
    "nlp_rule = spacy.load(\"rule_model_TOPS\")\n",
    "print(nlp_stat(content_test).ents)\n",
    "\n",
    "print(nlp_rule(content_test).ents)\n",
    "\n",
    "print(nlp(content_test).ents)\n",
    "\n",
    "displacy.render(nlp(content_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TOPS type rule based matcher --> tops_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type_matcher = c.create_patterns_matcher()\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "tops_patterns = c.create_tops_patterns()\n",
    "\n",
    "tops_matcher = Matcher(nlp.vocab, validate=True)\n",
    "#This rule_based matcher is only to detect \"TOPS\"\n",
    "tops_matcher.add(\"TOPS_TYPE\", tops_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = nlp(\"the tempo hoodie tops is the UPF 50+ activewear you've been looking for! It has thumbholes, a kangaroo pocket, and a hood for when the sun is too hot or you forgot your hat. Our Fitness hoodie tops is made out of our Active Athlon fabric with the added bonus of our Cooltect™ technology. You can be active in this fitted Fitness Hoodie TOPS without getting uncomfortably hot. So go ahead and enjoy sun-safe biking, walking, running and so much more!Highlights:UPF 50+Raglan long sleeves with thumbholesWelt kangaroo pocketHoodedActive Athlon™ fabric: Lightweight and breathable with moisture wicking for quick dry performanceCooltect™ technology accelerates moisture wicking to keep you cooler and more comfortable\")\n",
    "doc = nlp(content_test)\n",
    "tops_matcher(doc)\n",
    "for match_id, start, end in tops_matcher(doc):\n",
    "        print( doc[start:end].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. First we are trainning model to detect all products belonging to 'TOPS'\n",
    "(TODO: overlap type in matcher eg. \"t-shirt\" -> 'shirt' and 't-shirt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Solved) Overlap, duplicates, named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_data(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    #ignore for now \n",
    "    #detections = [(doc[start:end].start_char, doc[start:end].end_char, 'TOPS') for idx, start, end in type_matcher(doc) ]\n",
    "    \n",
    "    spans = [doc[start:end] for _, start, end in tops_matcher(doc)]\n",
    "    detections =  [(span.start_char, span.end_char, 'TOPS') for span in spacy.util.filter_spans(spans)] #remove duplicates or overlaps using spacy.util.filter_spans\n",
    "    \n",
    "    return (doc.text, {'entities': detections})\n",
    "\n",
    "#parse_train_data(\"top\") #testing, which should show the entities location\n",
    "#parse_train_data(\"These camisole and T-shirt and are so good. I did have similar pattern jacket which is so fancy. They all belong to tops. Gemi top, The top blue top is cloak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Next step: We need to operate sample dataset to seperate the 'product_type_number == 2' to create classifier of 'TOPS' trainning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_df = pd.read_csv('train_data/tops_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>product_type</th>\n",
       "      <th>tags</th>\n",
       "      <th>body_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mia Top, Ice Pinecones, Bamboo</td>\n",
       "      <td>top</td>\n",
       "      <td>{\"3/4 Sleeve\",333,50%,Bamboo,fw2020,fw2020repo...</td>\n",
       "      <td>DescriptionFlattering whether worn loose or bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ladybug Long Sleeve</td>\n",
       "      <td>tops</td>\n",
       "      <td>{_tab1_free-people-sizing,_tab2_atb-free-peopl...</td>\n",
       "      <td>Sweet corset-inspired long sleeve top featured...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pearl Top, Secret Garden, Bamboo</td>\n",
       "      <td>top</td>\n",
       "      <td>{20,Bamboo,Navy,sale,\"Short Sleeve\",Sunny,Top,...</td>\n",
       "      <td>Cross over top for all shapes. The dolman slee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHROMA SLEEVELESS HOODIE</td>\n",
       "      <td>premium sleeveless hoodie</td>\n",
       "      <td>{Nu,Psychedelic,\"Sacred Geometry\"}</td>\n",
       "      <td>Vibrant all over front, back &amp;amp; hood design...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rib Hacci Vagabond Tank</td>\n",
       "      <td>tops</td>\n",
       "      <td>{_tab1_zsupply-sizing,_tab2_atb-zsupply,_tab3_...</td>\n",
       "      <td>We updated our popular Vagabond tank! The Rib ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title               product_type  \\\n",
       "0    Mia Top, Ice Pinecones, Bamboo                        top   \n",
       "1               Ladybug Long Sleeve                       tops   \n",
       "2  Pearl Top, Secret Garden, Bamboo                        top   \n",
       "3          CHROMA SLEEVELESS HOODIE  premium sleeveless hoodie   \n",
       "4           Rib Hacci Vagabond Tank                       tops   \n",
       "\n",
       "                                                tags  \\\n",
       "0  {\"3/4 Sleeve\",333,50%,Bamboo,fw2020,fw2020repo...   \n",
       "1  {_tab1_free-people-sizing,_tab2_atb-free-peopl...   \n",
       "2  {20,Bamboo,Navy,sale,\"Short Sleeve\",Sunny,Top,...   \n",
       "3                 {Nu,Psychedelic,\"Sacred Geometry\"}   \n",
       "4  {_tab1_zsupply-sizing,_tab2_atb-zsupply,_tab3_...   \n",
       "\n",
       "                                           body_html  \n",
       "0  DescriptionFlattering whether worn loose or bo...  \n",
       "1  Sweet corset-inspired long sleeve top featured...  \n",
       "2  Cross over top for all shapes. The dolman slee...  \n",
       "3  Vibrant all over front, back &amp; hood design...  \n",
       "4  We updated our popular Vagabond tank! The Rib ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rules_based matcher, we can use the tops_total.csv directly to see how the matcher works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#text_type = pd.read_csv('sample_v1.csv', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\", \"product_type_number\"])\n",
    "\n",
    "#tops_df = text_type.loc[text_type['product_type_number'] == '2'].reset_index()\n",
    "#pd.concat([pd.DataFrame([i], columns=['label']) for i in range(70)])\n",
    "\n",
    "tops_df.insert(len(tops_df.columns), 'label', 1, allow_duplicates=True)\n",
    "#print(\"columns number : \", len(tops_df.columns))\n",
    "\n",
    "#prdiction(mixed_all) is based on prediction1 + prdiction 2 (all words) a & b\n",
    "tops_df.insert(len(tops_df.columns), 'prediction(mixed_all)', 0, allow_duplicates=True)\n",
    "#prediction_1 is based on tags + title + product_type\n",
    "tops_df.insert(len(tops_df.columns), 'prediction_1(title+pt+tags)', 0, allow_duplicates=True)\n",
    "#prediction_2 is based on product_description(body_html)\n",
    "tops_df.insert(len(tops_df.columns), 'prediction_2(body_html)', 0, allow_duplicates=True)\n",
    "\n",
    "#fill all empty cells \n",
    "tops_df.fillna(\"Not mention\", inplace=True)\n",
    "tops_df.insert(len(tops_df.columns), 'based_on_title', 0, allow_duplicates=True)\n",
    "tops_df.insert(len(tops_df.columns), 'based_on_tags', 0, allow_duplicates=True)\n",
    "tops_df.insert(len(tops_df.columns), 'based_on_product_type', 0, allow_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop all rows with each row 'title', 'tags' and 'product_type' to create new column called 'title+tag'\n",
    "\n",
    "errorcount = 0\n",
    "for i in range(len(tops_df)):\n",
    "    content_2_raw = ''\n",
    "    content_1_raw = ''\n",
    "    content_raw = ''\n",
    "    titles_raw = ''\n",
    "    tags_raw = ''\n",
    "    ptype_raw = ''\n",
    "    try:\n",
    "        content_1_raw = c.clean_tags_text(tops_df.loc[i, 'title'], tops_df.loc[i, 'product_type'], tops_df.loc[i, 'tags'])\n",
    "        \n",
    "        titles_raw = c.clean_tags_text(tops_df.loc[i, 'title'], None, None)\n",
    "        \n",
    "        tags_raw = c.clean_tags_text(None, None, tops_df.loc[i, 'tags'])\n",
    "        \n",
    "        ptype_raw = c.clean_tags_text(None, tops_df.loc[i, 'product_type'], None)\n",
    "        \n",
    "        content_2_raw = c.clean_product_description(tops_df.loc[i, 'body_html'])\n",
    "        \n",
    "        content_raw = content_1_raw + ', ' + content_2_raw\n",
    "        \n",
    "    except:\n",
    "        print(\"line type error \" + str(i) + ' '  + content_1_raw + '\\n' + titles_raw + '\\n' + tags_raw + '\\n' + ptype_raw + '\\n')\n",
    "    \n",
    "    content = nlp(content_raw)\n",
    "    content_1 = nlp(content_1_raw)\n",
    "    content_2 = nlp(content_2_raw)\n",
    "    titles = nlp(titles_raw)\n",
    "    tags = nlp(tags_raw)\n",
    "    ptype = nlp(ptype_raw)\n",
    "    \n",
    "    if len(tops_matcher(titles)) > 0:\n",
    "        tops_df.loc[i, 'based_on_title'] = 1\n",
    "    \n",
    "    if len(tops_matcher(tags)) > 0:\n",
    "        tops_df.loc[i, 'based_on_tags'] = 1\n",
    "        \n",
    "    if len(tops_matcher(ptype)) > 0:\n",
    "        tops_df.loc[i, 'based_on_product_type'] = 1   \n",
    "    \n",
    "    if len(tops_matcher(content_1)) > 0:\n",
    "        tops_df.loc[i, 'prediction_1(title+pt+tags)'] = 1\n",
    "        \n",
    "    if len(tops_matcher(content_2)) > 0:\n",
    "        tops_df.loc[i, 'prediction_2(body_html)'] = 1\n",
    "    \n",
    "    if len(tops_matcher(content)) > 0:\n",
    "        tops_df.loc[i, 'prediction(mixed_all)'] = 1\n",
    "    else:\n",
    "        errorcount += 1\n",
    "        print(errorcount, content_1_raw)\n",
    "        print(errorcount, content_2_raw)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. After every column using type_matcher, we update the 'prediction' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tops_df.rename(columns={\"index\": \"index_in_original_sample\"}, inplace=True)\n",
    "tops_df.to_csv('train_data/train_matcher_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_df.loc[tops_df['prediction(mixed_all)'] == 0]\n",
    "failed_TPT_df = tops_df.loc[tops_df['prediction_1(title+pt+tags)'] == 0, [\"title\", \"product_type\", \"tags\"]].reset_index()\n",
    "failed_TPT_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(failed_TPT_df.shape[0]):\n",
    "    s = c.clean_tags_text(failed_TPT_df.loc[i, 'title'], failed_TPT_df.loc[i, 'product_type'], failed_TPT_df.loc[i, 'tags'])\n",
    "    doc = nlp(s)\n",
    "    print(s)\n",
    "    displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tops_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/luis/Documents/GitHub/2022Summer/AI-Oriented-Recommendation-System/PreprocessingData/spaCy/spaCy_named_entity_recognition.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/2022Summer/AI-Oriented-Recommendation-System/PreprocessingData/spaCy/spaCy_named_entity_recognition.ipynb#ch0000020?line=0'>1</a>\u001b[0m \u001b[39m#print out the accuracy of each type matcher\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/2022Summer/AI-Oriented-Recommendation-System/PreprocessingData/spaCy/spaCy_named_entity_recognition.ipynb#ch0000020?line=1'>2</a>\u001b[0m rows_count \u001b[39m=\u001b[39m tops_df[tops_df\u001b[39m.\u001b[39mcolumns[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39mcount()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/2022Summer/AI-Oriented-Recommendation-System/PreprocessingData/spaCy/spaCy_named_entity_recognition.ipynb#ch0000020?line=2'>3</a>\u001b[0m all_mixed_prediction_correctness \u001b[39m=\u001b[39m (tops_df\u001b[39m.\u001b[39mloc[tops_df[\u001b[39m'\u001b[39m\u001b[39mprediction(mixed_all)\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m rows_count\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luis/Documents/GitHub/2022Summer/AI-Oriented-Recommendation-System/PreprocessingData/spaCy/spaCy_named_entity_recognition.ipynb#ch0000020?line=3'>4</a>\u001b[0m based_on_title_correctness \u001b[39m=\u001b[39m tops_df\u001b[39m.\u001b[39mloc[tops_df[\u001b[39m'\u001b[39m\u001b[39mbased_on_title\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m rows_count\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tops_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#print out the accuracy of each type matcher\n",
    "rows_count = tops_df[tops_df.columns[0]].count()\n",
    "all_mixed_prediction_correctness = (tops_df.loc[tops_df['prediction(mixed_all)'] == 1].shape[0]) / rows_count\n",
    "based_on_title_correctness = tops_df.loc[tops_df['based_on_title'] == 1].shape[0] / rows_count\n",
    "based_on_tags_correctness = tops_df.loc[tops_df['based_on_tags'] == 1].shape[0] / rows_count\n",
    "based_on_product_type_correctness = tops_df.loc[tops_df['based_on_product_type'] == 1].shape[0] / rows_count\n",
    "based_on_TagsTitleTags_correctness = tops_df.loc[tops_df['prediction_1(title+pt+tags)'] == 1].shape[0]/ rows_count\n",
    "based_on_body_html_correctness =tops_df.loc[tops_df['prediction_2(body_html)'] == 1].shape[0] / rows_count\n",
    "\n",
    "print(\"all_mixed_prediction_correctness : \", all_mixed_prediction_correctness)\n",
    "print(\"based_on_TagsTitleProductType_correctness\", based_on_TagsTitleTags_correctness)\n",
    "print(\"based_on_title_correctness : \", based_on_title_correctness)\n",
    "print(\"based_on_tags_correctness : \", based_on_tags_correctness)\n",
    "print(\"based_on_product_type_correctness : \", based_on_product_type_correctness)\n",
    "print(\"based_on_body_html_correctness : \", based_on_body_html_correctness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build TRIAN_DATA for 'tops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first trainning set is containing title+productType together (concatenate three)\n",
    "train_tops_df = pd.read_csv(\"train_data/train_matcher_result.csv\")\n",
    "#disgard the prediction_1(title+pt+tags) != 0 (unknown type)\n",
    "title_tags_type_df = train_tops_df.loc[train_tops_df['prediction_1(title+pt+tags)'] == 1, ['title',  'product_type', 'tags']].reset_index()\n",
    "title_tags_type_df.insert(len(title_tags_type_df.columns), 'raw_combined_text', '')\n",
    "for i in range(len(title_tags_type_df)):\n",
    "    try:\n",
    "        raw_combined_text = c.clean_tags_text(title_tags_type_df.loc[i, 'title'], title_tags_type_df.loc[i, 'product_type'], title_tags_type_df.loc[i, 'tags'])\n",
    "        title_tags_type_df.loc[i, 'raw_combined_text'] = raw_combined_text\n",
    "    except:\n",
    "        print(\"something wrong in line# : \", i)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags_type_df.loc[:,'raw_combined_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check our trian-dataframe title+tags+product_type\n",
    "TRAIN_DATA = [parse_train_data(d) for d in nlp.pipe(title_tags_type_df.loc[:,'raw_combined_text'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA [5:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINNING LOOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_blank_nlp(train_data):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    #ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            try:\n",
    "                ner.add_label(ent[2])#'label' tops, ent[0], ent[1] are start_char and end_char\n",
    "            except:\n",
    "                print(ent[2])\n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime as dt\n",
    "from spacy.training import Example\n",
    "nlp = creat_blank_nlp(TRAIN_DATA)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(50):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    for text, annotation in TRAIN_DATA:\n",
    "        try:\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotation)\n",
    "            nlp.update([example], sgd=optimizer, losses=losses)\n",
    "        except:\n",
    "            print(\"Error happens on : \", text, annotation)\n",
    "    print(f\"Losses at iteration {i} - {dt.datetime.now()}\", losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I wear a fancy T-SHirt and I got another button-down wonderful crop tee .     Long shirt and coat are necessary for keeping warm in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    winter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".     \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sweater\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and blouse are important for people living in the north.     \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " students have their own hoodies. The Tank Top is new stylish top-clothes.     What about trying our new camisole which is fantastic? </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from spacy import displacy\n",
    "\n",
    "#description is not accurate\n",
    "s = \"I wear a fancy T-SHirt and I got another button-down wonderful crop tee . \\\n",
    "    Long shirt and coat are necessary for keeping warm in winter. \\\n",
    "    Sweater and blouse are important for people living in the north. \\\n",
    "    UA students have their own hoodies. The Tank Top is new stylish top-clothes. \\\n",
    "    What about trying our new camisole which is fantastic? \"    \n",
    "doc2 = nlp(s)\n",
    "displacy.render(doc2, style='ent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"There are certain pieces that will always bring a boho style aesthetic to mind, and this boxy top is one of them. \\\n",
    "    It's crafted from an open floral crochet with a scalloped hem and short sleeves. \\\n",
    "    We're showcasing the circle crochet trim along the round neckline with a turquoise necklace to really knock it out of the park. \"\n",
    "doc3 = nlp(s)\n",
    "displacy.render(doc3, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = \"The Tempo Hoodie is the UPF 50+ activewear you've been looking for! \\\n",
    "    It has thumbholes, a kangaroo pocket, and a hood for when the sun is too hot or you forgot your hat. \\\n",
    "    Our Fitness Hoodie is made out of our Active Athlon fabric with the added bonus of our Cooltect™ technology. \\\n",
    "    You can be active in this fitted Fitness Hoodie without getting uncomfortably hot. \\\n",
    "    So go ahead and enjoy sun-safe biking, walking, running and so much more!Highlights:UPF 50+Raglan long sleeves with thumbholesWelt \\\n",
    "    kangaroo pocketHoodedActive Athlon™ fabric: Lightweight and breathable with moisture wicking for quick dry performanceCooltect™ technology \\\n",
    "    accelerates moisture wicking to keep you cooler and more comfortable\"\n",
    "doc4 = nlp(s4)\n",
    "displacy.render(doc4, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5 = \"\"\n",
    "s5 = c.clean_tags_text(\"Fox Men's hoodie\", \"mens clothing\", \"{}\")\n",
    "doc5 = nlp(s5)\n",
    "displacy.render(doc5, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline #We start with 'empty' pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"ML_based_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Next step I will try to add more pipelines like tokenizer, tagger, parser, ner... \n",
    "label \n",
    "        ---> model ----> rules\n",
    "        \n",
    "data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08f33c96bfe48976e0772c3a4097c2fff4477ea59ec4556e2e8fa1251315c612"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
