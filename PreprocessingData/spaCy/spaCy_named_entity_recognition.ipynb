{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common as c\n",
    "\n",
    "#The codes below and next cell are for specific example testing.\n",
    "raw_tag = '{BAGGU,groupbycolor,\"home goods\",Textiles}'\n",
    "raw_title = 'Reusable Cloth Set / Backyard Fruit'\n",
    "raw_product_type = 'textiles'\n",
    "#line 31 example\n",
    "raw_product_description = 'You may find yourself in a sticky situation. \\\n",
    "    You may need to secure that mushroom you came across in the woods. \\\n",
    "    You may need a tablecloth for a very small table. \\\n",
    "    You may have a runny nose or cold neck or be having a bad hair day and just want to cover it up. \\\n",
    "    You may ask yourself why you left home without a square of fabric. \\\n",
    "    You may have just found the solution â€” 20 square inches of pure possibility.\\\n",
    "    Details:Set of threeMeasures 20'' H x 20'' W100% Organic Cotton\\\n",
    "    About the Brand:Founded in 2007, BAGGU set out to create a reusable bag that was as functional as it was adorable. \\\n",
    "    Today their goal is to make every bag you need for your every day life. \\\n",
    "    They have stuck with their mission of creating useful products that are made with you and the planet in mind. \\\n",
    "    BAGGU is manufactured ethically and environmentally responsibly in China.'\n",
    "#pre-process the tag/title/product_type\n",
    "content_1 = c.clean_tags_text(raw_tag, raw_title, raw_product_type)\n",
    "#pre-process the production_description\n",
    "content_2 = c.clean_product_description(raw_product_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_matcher = c.create_patterns_matcher()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. First we are trainning model to detect all products belonging to 'TOPS'\n",
    "(TODO: overlap type in matcher eg. \"t-shirt\" -> 'shirt' and 't-shirt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Solved) Overlap, duplicates, named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('These camisole and T-shirt and are so good. I did have similar pattern jacket which is so fancy',\n",
       " {'entities': [(6, 14, 'TOPS'), (19, 26, 'TOPS'), (71, 77, 'TOPS')]})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_train_data(text):\n",
    "    doc = nlp(text)\n",
    "    #ignore for now \n",
    "    #detections = [(doc[start:end].start_char, doc[start:end].end_char, 'TOPS') for idx, start, end in type_matcher(doc) ]\n",
    "    \n",
    "    spans = [doc[start:end] for _, start, end in type_matcher(doc)]\n",
    "    detections =  [(span.start_char, span.end_char, 'TOPS') for span in spacy.util.filter_spans(spans)] #remove duplicates or overlaps using spacy.util.filter_spans\n",
    "    \n",
    "    return (doc.text, {'entities': detections})\n",
    "\n",
    "#parse_train_data(\"I like my jacket\")\n",
    "parse_train_data(\"These camisole and T-shirt and are so good. I did have similar pattern jacket which is so fancy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Next step: We need to operate sample dataset to seperate the 'product_type_number == 2' to create classifier of 'TOPS' trainning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_type = pd.read_csv('sample_v1.csv', usecols=[\"title\", \"tags\", \"product_type\", \"body_html\", \"product_type_number\"])\n",
    "\n",
    "tops_df = text_type.loc[text_type['product_type_number'] == '2'].reset_index()\n",
    "#pd.concat([pd.DataFrame([i], columns=['label']) for i in range(70)])\n",
    "\n",
    "tops_df.insert(len(tops_df.columns), 'label', 1, allow_duplicates=True)\n",
    "#print(\"columns number : \", len(tops_df.columns))\n",
    "\n",
    "#prdiction(mixed_all) is based on prediction1 + prdiction 2 (all words) a & b\n",
    "tops_df.insert(len(tops_df.columns), 'prediction(mixed_all)', 0, allow_duplicates=True)\n",
    "#prediction_1 is based on tags + title + product_type\n",
    "tops_df.insert(len(tops_df.columns), 'prediction_1(title+pt+tags)', 0, allow_duplicates=True)\n",
    "#prediction_2 is based on product_description(body_html)\n",
    "tops_df.insert(len(tops_df.columns), 'prediction_2(body_html)', 0, allow_duplicates=True)\n",
    "\n",
    "#fill all empty cells \n",
    "tops_df.fillna(\"Not mention\", inplace=True)\n",
    "tops_df.insert(len(tops_df.columns), 'based_on_title', 0, allow_duplicates=True)\n",
    "tops_df.insert(len(tops_df.columns), 'based_on_tags', 0, allow_duplicates=True)\n",
    "tops_df.insert(len(tops_df.columns), 'based_on_product_type', 0, allow_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop all rows with each row 'title', 'tags' and 'product_type' to create new column called 'title+tag'\n",
    "type_matcher = c.create_patterns_matcher()\n",
    "errorcount = 0\n",
    "for i in range(len(tops_df)):\n",
    "    content_2_raw = ''\n",
    "    content_1_raw = ''\n",
    "    content_raw = ''\n",
    "    titles_raw = ''\n",
    "    tags_raw = ''\n",
    "    ptype_raw = ''\n",
    "    try:\n",
    "        content_1_raw = c.clean_tags_text(tops_df.loc[i, 'tags'], tops_df.loc[i, 'title'], tops_df.loc[i, 'product_type'])\n",
    "        \n",
    "        titles_raw = c.clean_tags_text(None, tops_df.loc[i, 'title'], None)\n",
    "        \n",
    "        tags_raw = c.clean_tags_text(tops_df.loc[i, 'tags'], None, None)\n",
    "        \n",
    "        ptype_raw = c.clean_tags_text(None, None,  tops_df.loc[i, 'product_type'])\n",
    "        \n",
    "        content_2_raw = c.clean_product_description(tops_df.loc[i, 'body_html'])\n",
    "        \n",
    "        content_raw = content_1_raw + ', ' + content_2_raw\n",
    "        \n",
    "    except:\n",
    "        print(\"line type error \",i)\n",
    "    \n",
    "    content = nlp(content_raw)\n",
    "    content_1 = nlp(content_1_raw)\n",
    "    content_2 = nlp(content_2_raw)\n",
    "    titles = nlp(titles_raw)\n",
    "    tags = nlp(tags_raw)\n",
    "    ptype = nlp(ptype_raw)\n",
    "    \n",
    "    if len(type_matcher(titles)) > 0:\n",
    "        tops_df.loc[i, 'based_on_title'] = 1\n",
    "    \n",
    "    if len(type_matcher(tags)) > 0:\n",
    "        tops_df.loc[i, 'based_on_tags'] = 1\n",
    "        \n",
    "    if len(type_matcher(ptype)) > 0:\n",
    "        tops_df.loc[i, 'based_on_product_type'] = 1   \n",
    "    \n",
    "    if len(type_matcher(content_1)) > 0:\n",
    "        tops_df.loc[i, 'prediction_1(title+pt+tags)'] = 1\n",
    "        \n",
    "    if len(type_matcher(content_2)) > 0:\n",
    "        tops_df.loc[i, 'prediction_2(body_html)'] = 1\n",
    "    \n",
    "    if len(type_matcher(content)) > 0:\n",
    "        tops_df.loc[i, 'prediction(mixed_all)'] = 1\n",
    "    else:\n",
    "        errorcount += 1\n",
    "        print(errorcount, content_1_raw)\n",
    "        print(errorcount, content_2_raw)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. After every column using type_matcher, we update the 'prediction' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_df.rename(columns={\"index\": \"index_in_original_sample\"}, inplace=True)\n",
    "tops_df.to_csv('TOPS_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_in_original_sample</th>\n",
       "      <th>title</th>\n",
       "      <th>product_type</th>\n",
       "      <th>tags</th>\n",
       "      <th>product_type_number</th>\n",
       "      <th>body_html</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction(mixed_all)</th>\n",
       "      <th>prediction_1(title+pt+tags)</th>\n",
       "      <th>prediction_2(body_html)</th>\n",
       "      <th>based_on_title</th>\n",
       "      <th>based_on_tags</th>\n",
       "      <th>based_on_product_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_in_original_sample, title, product_type, tags, product_type_number, body_html, label, prediction(mixed_all), prediction_1(title+pt+tags), prediction_2(body_html), based_on_title, based_on_tags, based_on_product_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops_df.loc[tops_df['prediction(mixed_all)'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_mixed_prediction_correctness :  1.0\n",
      "based_on_TagsTitleProductType_correctness 1.0\n",
      "based_on_title_correctness :  0.8048780487804879\n",
      "based_on_tags_correctness :  0.5487804878048781\n",
      "based_on_product_type_correctness :  0.6463414634146342\n",
      "based_on_body_html_correctness :  0.6097560975609756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print out the accuracy of each type matcher\n",
    "rows_count = tops_df[tops_df.columns[0]].count()\n",
    "all_mixed_prediction_correctness = (tops_df.loc[tops_df['prediction(mixed_all)'] == 1].shape[0]) / rows_count\n",
    "based_on_title_correctness = tops_df.loc[tops_df['based_on_title'] == 1].shape[0] / rows_count\n",
    "based_on_tags_correctness = tops_df.loc[tops_df['based_on_tags'] == 1].shape[0] / rows_count\n",
    "based_on_product_type_correctness = tops_df.loc[tops_df['based_on_product_type'] == 1].shape[0] / rows_count\n",
    "based_on_TagsTitleTags_correctness = tops_df.loc[tops_df['prediction_1(title+pt+tags)'] == 1].shape[0]/ rows_count\n",
    "based_on_body_html_correctness =tops_df.loc[tops_df['prediction_2(body_html)'] == 1].shape[0] / rows_count\n",
    "\n",
    "print(\"all_mixed_prediction_correctness : \", all_mixed_prediction_correctness)\n",
    "print(\"based_on_TagsTitleProductType_correctness\", based_on_TagsTitleTags_correctness)\n",
    "\n",
    "print(\"based_on_title_correctness : \", based_on_title_correctness)\n",
    "print(\"based_on_tags_correctness : \", based_on_tags_correctness)\n",
    "print(\"based_on_product_type_correctness : \", based_on_product_type_correctness)\n",
    "print(\"based_on_body_html_correctness : \", based_on_body_html_correctness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build TRIAN_DATA for 'tops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first trainning set is containing title+productType together (concatenate three)\n",
    "\n",
    "title_tags_type_df = tops_df[['title', 'tags', 'product_type']]\n",
    "title_tags_type_df.insert(len(title_tags_type_df.columns), 'raw_combined_text', '')\n",
    "for i in range(len(title_tags_type_df)):\n",
    "    try:\n",
    "        raw_combined_text = c.clean_tags_text(title_tags_type_df.loc[i, 'tags'], title_tags_type_df.loc[i, 'title'], title_tags_type_df.loc[i, 'product_type'])\n",
    "        \n",
    "        title_tags_type_df.loc[i, 'raw_combined_text'] = raw_combined_text\n",
    "    except:\n",
    "        print(\"something wrong in line# : \", i)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     nu psychedelic, word is world sports bra, spor...\n",
       "1     free people tanks tops womens, scoop me up rac...\n",
       "2     mens national standard s/s t tops white, tribl...\n",
       "3     nu psychedelic, abstract waves black string bi...\n",
       "4     ready to ship, bees sweatshirt (clearance), re...\n",
       "                            ...                        \n",
       "77    basic carryover new regular sleeveless ss2021 ...\n",
       "78    20 bamboo pink sale ss2020 sunny top, peace to...\n",
       "79    _tab1_daydreamer-sizing _tab2_atb-daydreamer _...\n",
       "80    20 3/4 sleeve blouse carryover long sleeve ray...\n",
       "81    cotopaxi outerwear womens, cotopaxi women's te...\n",
       "Name: raw_combined_text, Length: 82, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tags_type_df.loc[:,'raw_combined_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check our trian-dataframe title+tags+product_type\n",
    "TRAIN_DATA = [parse_train_data(d) for d in nlp.pipe(title_tags_type_df.loc[:,'raw_combined_text'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('animals nu psychedelic, acid gorilla hoodie, premium hoodie',\n",
       "  {'entities': [(37, 43, 'TOPS'), (53, 59, 'TOPS')]}),\n",
       " ('groupbycolor stateside tops, lounge l/s boat neck \" lightweight rib\" / black, tops',\n",
       "  {'entities': [(23, 27, 'TOPS'), (78, 82, 'TOPS')]}),\n",
       " ('_tab1_free-people-sizing _tab2_atb-free-people _tab3_care-free-people blouse clothing free-people tops, check on it wrap top, tops',\n",
       "  {'entities': [(70, 76, 'TOPS'),\n",
       "    (77, 85, 'TOPS'),\n",
       "    (98, 102, 'TOPS'),\n",
       "    (121, 124, 'TOPS'),\n",
       "    (126, 130, 'TOPS')]})]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA [5:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINNING LOOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_blank_nlp(train_data):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            try:\n",
    "                ner.add_label(ent[2])#'label' tops, ent[0], ent[1] are start_char and end_char\n",
    "            except:\n",
    "                print(ent[2])\n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0 - 2022-05-23 21:00:15.829118 {'ner': 314.43091249386043}\n",
      "Losses at iteration 1 - 2022-05-23 21:00:16.816551 {'ner': 41.87719376894409}\n",
      "Losses at iteration 2 - 2022-05-23 21:00:17.797378 {'ner': 62.68215404425492}\n",
      "Losses at iteration 3 - 2022-05-23 21:00:18.776701 {'ner': 46.93266654468059}\n",
      "Losses at iteration 4 - 2022-05-23 21:00:19.697314 {'ner': 12.1619733861654}\n",
      "Losses at iteration 5 - 2022-05-23 21:00:20.670280 {'ner': 26.378976128524783}\n",
      "Losses at iteration 6 - 2022-05-23 21:00:21.634569 {'ner': 7.085080688516168}\n",
      "Losses at iteration 7 - 2022-05-23 21:00:22.585445 {'ner': 9.90187884219191}\n",
      "Losses at iteration 8 - 2022-05-23 21:00:23.565554 {'ner': 1.2482176453512215}\n",
      "Losses at iteration 9 - 2022-05-23 21:00:24.522437 {'ner': 2.064205024468288}\n",
      "Losses at iteration 10 - 2022-05-23 21:00:25.469443 {'ner': 3.6830311851043436}\n",
      "Losses at iteration 11 - 2022-05-23 21:00:26.381656 {'ner': 5.751324400664479}\n",
      "Losses at iteration 12 - 2022-05-23 21:00:27.303555 {'ner': 9.502543072578776}\n",
      "Losses at iteration 13 - 2022-05-23 21:00:28.265179 {'ner': 1.982580604605806}\n",
      "Losses at iteration 14 - 2022-05-23 21:00:29.199970 {'ner': 4.163228296650126}\n",
      "Losses at iteration 15 - 2022-05-23 21:00:30.119356 {'ner': 0.5593612990889812}\n",
      "Losses at iteration 16 - 2022-05-23 21:00:31.037535 {'ner': 4.428096292987671}\n",
      "Losses at iteration 17 - 2022-05-23 21:00:31.973231 {'ner': 3.9758329933487673}\n",
      "Losses at iteration 18 - 2022-05-23 21:00:32.904305 {'ner': 0.6073971479535751}\n",
      "Losses at iteration 19 - 2022-05-23 21:00:33.832151 {'ner': 1.683846341729801}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import datetime as dt\n",
    "from spacy.training import Example\n",
    "nlp = creat_blank_nlp(TRAIN_DATA)\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    for text, annotation in TRAIN_DATA:\n",
    "        try:\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotation)\n",
    "            nlp.update([example], sgd=optimizer, losses=losses)\n",
    "        except:\n",
    "            print(\"Error happens on : \", text, annotation)\n",
    "    print(f\"Losses at iteration {i} - {dt.datetime.now()}\", losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I wear a fancy \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    T-SHirt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " and I got another button-down wonderful \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    crop\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tee\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " Long \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    shirt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    coat\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " are necessary for keeping warm in winter. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sweater\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    blouse\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " are important for people living in the north. UA students have their own \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hoodies\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tank\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " Top is new stylish top-clothes. What about trying our new camisole which is fantastic? </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from spacy import displacy\n",
    "\n",
    "#description is not accurate\n",
    "s = \"I wear a fancy T-SHirt and I got another button-down wonderful crop tee . Long shirt and coat are necessary for keeping warm in winter. Sweater and blouse are important for people living in the north. UA students have their own hoodies. The Tank Top is new stylish top-clothes. What about trying our new camisole which is fantastic? \"    \n",
    "doc2 = nlp(s)\n",
    "displacy.render(doc2, style='ent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">There are certain pieces that will always bring a boho style aesthetic to mind, and this boxy \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    top\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " is one of them.     It's crafted from an open floral crochet with a scalloped hem and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    short\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TOPS</span>\n",
       "</mark>\n",
       " sleeves.     We're showcasing the circle crochet trim along the round neckline with a turquoise necklace to really knock it out of the park. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"There are certain pieces that will always bring a boho style aesthetic to mind, and this boxy top is one of them. \\\n",
    "    It's crafted from an open floral crochet with a scalloped hem and short sleeves. \\\n",
    "    We're showcasing the circle crochet trim along the round neckline with a turquoise necklace to really knock it out of the park. \"\n",
    "doc3 = nlp(s)\n",
    "displacy.render(doc3, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ner', <spacy.pipeline.ner.EntityRecognizer at 0x28a832e40>)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline #We start with 'empty' pipeline, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Next step I will try to add more pipelines like tokenizer, tagger, parser, ner... \n",
    "label \n",
    "        ---> model ----> rules\n",
    "data   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08f33c96bfe48976e0772c3a4097c2fff4477ea59ec4556e2e8fa1251315c612"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
